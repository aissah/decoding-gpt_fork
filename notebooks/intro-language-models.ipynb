{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import unicodedata\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import base64\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import torch\n",
    "import numpy as np\n",
    "from jaxtyping import Int, Float\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mm(graph):\n",
    "  \"\"\"for plotting mermaid.js diagrams\"\"\"\n",
    "  graphbytes = graph.encode(\"ascii\")\n",
    "  base64_bytes = base64.b64encode(graphbytes)\n",
    "  base64_string = base64_bytes.decode(\"ascii\")\n",
    "  display(\n",
    "    Image(\n",
    "      url=\"https://mermaid.ink/img/\"\n",
    "      + base64_string\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we model language? Markov Chains!\n",
    "\n",
    "The most basic way to model language is what your phone's predictive text uses. It's called a *Markov Chain,* and it assumes the *Markov Property:*\n",
    "\n",
    "*given a process which can move between some set of states $s_1, s_2, \\ldots s_n$, the probability of moving to the next state depends only on the current state and not on the sequence of events that preceded it.*\n",
    "\n",
    "> Question: What are some examples of processes that this is a good and a bad model for?\n",
    "\n",
    "A Markov Chain then takes the form of a matrix $M$ where $M_{i,j}$ is the probability of moving from state $i$ to state $j$.\n",
    "\n",
    "- All matrix entries must be between 0 and 1\n",
    "- Each row must sum to 1 (the probability of moving somewhere)\n",
    "- The matrix must be square, with the same number of rows and columns as there are states\n",
    "\n",
    "The entries of the matrix give us the probability that, given that the previous state was $s_i$, the next state will be $s_j$:\n",
    "\n",
    "$$ M_{i,j} := P[x_{t+1} = s_j | x_t = s_i] $$\n",
    "\n",
    "Let's look at an example. Suppose we have a language with only 3 words: \"I\", \"like\", and \"cheese\". The only possible sentence in this language is \"I like cheese\". We can model this language with the following Markov Chain:\n",
    "\n",
    "|        | I   | like | cheese |\n",
    "| ------ | --- | ---- | ------ |\n",
    "| I      | 0   | 1    | 0      |\n",
    "| like   | 0   | 0    | 1      |\n",
    "| cheese | 1   | 0    | 0      |\n",
    "\n",
    "\n",
    "To have a markov chain generate a sequence of states, we start with some initial state $x_0$, and then we *sample* from the distribution of the next state given the current state:\n",
    "\n",
    "$$ x_{t+1} \\sim P[x_{t+1} | x_0] $$\n",
    "\n",
    "We can then append this to our sequence, and repeat the process.\n",
    "\n",
    "Let's put this into some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cheese_vocab = [\"I\", \"like\", \"cheese\"]\n",
    "cheese_chain = np.array([\n",
    "\t[0, 1, 0],\n",
    "\t[0, 0, 1],\n",
    "\t[1, 0, 0],\n",
    "])\n",
    "\n",
    "initial_state = \"I\"\n",
    "initial_state_idx = cheese_vocab.index(initial_state)\n",
    "\n",
    "next_state_probabilities = cheese_chain[initial_state_idx]\n",
    "\n",
    "np.random.choice(cheese_vocab, p=next_state_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, (without looking ahead in the code!), try to implement a function which generates an entire sequence of states. Don't forget to add a stopping condition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "some_text = \"\"\"\n",
    "In reality, of course, we don't construct such chains explicitly, but instead we want them to learn from data.\n",
    "\n",
    "To put something in a markov chain or neural network, we need to turn it into numbers. this is straightforward for images: each pixel is already a number!\n",
    "\n",
    "In computers, text is stored as a sequence of numbers. Our neural network, in principle, can learn to predict the next number in the sequence. However, each number usually represents a single letter, or even just part of a letter. what do you think happens when we throw something like this into a markov chain?\n",
    "\"\"\"\n",
    "\n",
    "some_text = \"The quick red fox jumps over the lazy brown dog. The quick red fox jumps over the lazy brown dog.\" + some_text\n",
    "\n",
    "def clean_text(text: str) -> list[str]:\n",
    "\treturn [x for x in text.lower() if x.isalpha() or x == ' ']\n",
    "\n",
    "some_text_chars = clean_text(some_text)\n",
    "char_vocab = sorted(set(some_text_chars))\n",
    "print(char_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# let's train a markov chain on this text!\n",
    "\n",
    "def train_char_markov_chain(\n",
    "\ttext: str,\n",
    "\tvocab: list[str],\n",
    ") -> Float[np.ndarray, \"d_vocab d_vocab\"]:\n",
    "\n",
    "\ttext = \"\".join(clean_text(text))\n",
    "\n",
    "\tvocab_size = len(vocab)\n",
    "\tchain: Float[np.ndarray, \"d_vocab d_vocab\"] = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "\tfor i in range(len(text) - 1):\n",
    "\t\tchar1 = text[i]\n",
    "\t\tchar2 = text[i + 1]\n",
    "\t\tchain[vocab.index(char1), vocab.index(char2)] += 1\n",
    "\t\n",
    "\tchain /= chain.sum(axis=1, keepdims=True)\n",
    "\n",
    "\treturn chain\n",
    "\n",
    "\n",
    "char_chain = train_char_markov_chain(some_text, char_vocab)\n",
    "\n",
    "def plot_char_chain(chain: Float[np.ndarray, \"d_vocab d_vocab\"]):\n",
    "\tplt.matshow(chain)\n",
    "\t# add vocab labels\n",
    "\tplt.xticks(range(len(char_vocab)), char_vocab)\n",
    "\tplt.yticks(range(len(char_vocab)), char_vocab)\n",
    "\tplt.show()\n",
    "\n",
    "plot_char_chain(char_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what does this chain predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def char_generate(\n",
    "\tprompt: str,\n",
    "\tchain: Float[np.ndarray, \"d_vocab d_vocab\"],\n",
    "\tvocab: list[str],\n",
    "\tmax_new_chars: int = 100,\n",
    ") -> list[str]:\n",
    "\n",
    "\toutput = list(prompt)\n",
    "\n",
    "\tfor i in range(max_new_chars):\n",
    "\t\tc = output[-1]\n",
    "\t\tc_idx = vocab.index(c)\n",
    "\t\tnext_letter_probabilities = chain[c_idx]\n",
    "\t\tc_next = np.random.choice(vocab, p=next_letter_probabilities)\n",
    "\t\t# c_next = vocab[next_letter_probabilities.argmax()]\n",
    "\t\toutput.append(c_next)\n",
    "\n",
    "\treturn \"\".join(output)\n",
    "\n",
    "print(char_generate(\"the \", char_chain, char_vocab, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, clearly, that doesn't work great. What can we learn from that markov chain though?\n",
    "\n",
    "In particular, what happens when we keep running it forever?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did previously was assume a single initial state. However, we can also assume a *distribution* over initial states. If we, for example, assume a uniform distribution $p$ over the initial states:\n",
    "\n",
    "$$ p[i] = \\frac{1}{d_{\\texttt{vocab}}} $$\n",
    "\n",
    "then we can just multiply our matrix by this distribution to get the distribution over the next state:\n",
    "\n",
    "$$ p_{t+1} = M p_t $$\n",
    "\n",
    "So what happens as we take the iteration $t \\to \\infty$? If $M^\\infty$ converges, then we can just say\n",
    "\n",
    "$$ p_* = M^\\infty p_* = M p_* $$\n",
    "\n",
    "does that remind you of anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of an eigenvector $v$ with eigenvalue $\\lambda$ for some matrix $A$:\n",
    "\n",
    "$$ A v = \\lambda v $$\n",
    "\n",
    "In our case, $\\lambda = 1$, and $v = p_*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def compute_chain_eig(\n",
    "\tchain: Float[np.ndarray, \"d_vocab d_vocab\"],\n",
    "):\n",
    "\teigvals, eigvecs = np.linalg.eig(chain)\n",
    "\treturn eigvals, eigvecs\n",
    "\n",
    "eigvals, eigvecs = compute_chain_eig(char_chain)\n",
    "\n",
    "for eL, eV in zip(eigvals, eigvecs):\n",
    "\t# check the real part of the eigenvalue is 1 and the imaginary part is 0\n",
    "\tif np.isclose(eL.real, 1) and np.isclose(eL.imag, 0):\n",
    "\t\tfor c, v in zip(char_vocab, eV):\n",
    "\t\t\t# print the magnitude of the eigenvector\n",
    "\t\t\tprint(f\"{c}: {v.real:.3f}\")\n",
    "\t\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plot_char_chain(np.linalg.matrix_power(char_chain, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be asking what any of this markov chain stuff has to do with neural networks -- but we can actually think of a markov chain as a single-layer neural network (with no bias):\n",
    "\n",
    "$$\n",
    "\ty = M x\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph LR;\n",
    "\tin([input])\n",
    "\tlin[[Linear]]\n",
    "\tout([output])\n",
    "\tin-->lin;\n",
    "\tlin-->out;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise: Markov Chain as a Neural Network\n",
    "Implement a markov chain as a pytorch module, train it, and compare the weights you get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "class MarkovChain(torch.nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tvocab_arr: list[str],\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.vocab_arr: list[str] = vocab_arr\n",
    "\t\tself.d_vocab: int = len(vocab_arr)\n",
    "\t\tself.vocab_dict: dict[str, int] = {c: i for i, c in enumerate(vocab_arr)}\n",
    "\n",
    "\t\tself.linear: torch.nn.Module = None # TODO\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx: Int[torch.Tensor, \"d_vocab\"],\n",
    "\t) -> Float[torch.Tensor, \"d_vocab\"]:\n",
    "\t\toutput = None # TODO\n",
    "\t\treturn output\n",
    "\t\n",
    "\tdef generate(\n",
    "\t\tself,\n",
    "\t\tprompt: list[str],\n",
    "\t\tmax_new_chars: int = 100,\n",
    "\t) -> str:\n",
    "\t\toutput: list[str] = prompt\n",
    "\n",
    "\t\tfor i in range(max_new_chars):\n",
    "\t\t\tc: str = None # TODO\n",
    "\t\t\tc_idx: int = None # TODO\n",
    "\t\t\tnext_letter_probabilities: Float[torch.Tensor, \"d_vocab\"] = # TODO\n",
    "\t\t\tc_next: str = None # TODO\n",
    "\t\t\toutput.append(c_next)\n",
    "\n",
    "\t\treturn \"\".join(output)\n",
    "\n",
    "def train_torch_chain(\n",
    "\tdata: list[str],\n",
    "\tvocab: list[str],\n",
    "\tn_epochs: int = 100,\n",
    "\tbatch_size: int = 32,\n",
    ") -> MarkovChain:\n",
    "\t\n",
    "\tchain: MarkovChain = MarkovChain(vocab)\n",
    "\toptimizer = torch.optim.Adam(chain.parameters(), lr=1e-3)\n",
    "\tloss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\tfor e in range(n_epochs):\n",
    "\t\tfor i in range(0, len(data) // batch_size): # this skips the last batch if it's not the right size, whatever\n",
    "\t\t\tbatch = data[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "\t\t\tpredictions = None # TODO\n",
    "\t\t\ttargets = None\n",
    "\n",
    "\n",
    "\treturn chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "In practice, character-level encoding comes with a tradeoff: you have a very small vocabulary, but you need a very long sequence to encode any meaningful information.\n",
    "We want to give our system a useful *prior:* most of the time, in natural language, letters only appear in certain sequences which we call *words.*\n",
    "\n",
    "The simplest way to tokenize is just to split the text by spaces into words (treating all punctuation as a unique word). This gives you some weird looking sequences, but its a good start.\n",
    "\n",
    "If we want to explore tokenization, we first need a real dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data\n",
    "gutenberg books has a lot of public domain books. the code below will download a selected few books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[str]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "DATA_RAW: list[str] = get_many_books([84, 15, 18, 82, 996, 2600])\n",
    "\n",
    "print(f\"{sum(len(x) for x in DATA_RAW) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "f\"{DATA_RAW[0][10_000:10_500]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing data\n",
    "\n",
    "as you can see, the above text has a lot of things we don't *really* care all that much about. Things like whitespace, punctuation, and capitalization. We can remove these things to make our lives easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def process_text(\n",
    "\ttext: str,\n",
    "\tallowed_punctuation: str = \"-.,;:!?()\\\"\" + \"\".join(str(x) for x in range(10)),\n",
    "\tpunctuation_convert: dict[str,str] = {'—': '-'},\n",
    ") -> str:\n",
    "\t\n",
    "\t# replace some special characters which unicode won't normalize properly\n",
    "\tfor char, replacement in punctuation_convert.items():\n",
    "\t\ttext = text.replace(char, replacement)\n",
    "\n",
    "\t# if a line has \".jpg\" in it, remove that line (this is specific to Don Quixote)\n",
    "\ttext = '\\n'.join(\n",
    "\t\tline \n",
    "\t\tfor line in text.split('\\n')\n",
    "\t\tif '.jpg' not in line\n",
    "\t)\n",
    "\n",
    "\t# Normalize the string to decompose Unicode characters\n",
    "\ttext = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "\t# Encode to ASCII bytes, then decode back to string, ignoring errors\n",
    "\ttext = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\t# remove newlines and tabs\n",
    "\ttext = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "\n",
    "\t# put spaces around allowed punctuation\n",
    "\tfor char in allowed_punctuation:\n",
    "\t\ttext = text.replace(char, f' {char} ')\n",
    "\n",
    "\n",
    "\t# remove leading and trailing spaces\n",
    "\ttext = text.strip()\n",
    "\n",
    "\t# remove multiple spaces\n",
    "\twhile '  ' in text:\n",
    "\t\ttext = text.replace('  ', ' ')\n",
    "\n",
    "\n",
    "\t# remove all characters except (alphanumeric, allowed_punctuation, ' ')\n",
    "\ttext = ''.join(\n",
    "\t\t(\n",
    "\t\t\tchar \n",
    "\t\t\tif (\n",
    "\t\t\t\tchar.isalnum() \n",
    "\t\t\t\tor char in allowed_punctuation \n",
    "\t\t\t\tor char == ' '\n",
    "\t\t\t)\n",
    "\t\t\telse ' '\n",
    "\t\t)\n",
    "\t\tfor char in text \n",
    "\t)\n",
    "\n",
    "\t# convert to lowercase\n",
    "\ttext = text.lower()\n",
    "\n",
    "\ttext = text.strip()\n",
    "\n",
    "\treturn text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "DATA: str = \" \".join(process_text(x) for x in DATA_RAW)\n",
    "\n",
    "print(DATA[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## character-level chain\n",
    "\n",
    "just for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "char_chain_bigdata = train_char_markov_chain(DATA, char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.matshow(char_chain_bigdata)\n",
    "# add vocab labels\n",
    "plt.xticks(range(len(char_vocab)), char_vocab)\n",
    "plt.yticks(range(len(char_vocab)), char_vocab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming up with a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def tokenize(\n",
    "\ttext: str,\n",
    "\tprocess: bool = False,\n",
    ") -> list[str]:\n",
    "\tif process:\n",
    "\t\ttext = process_text(text)\n",
    "\treturn text.split(' ')\n",
    "\n",
    "DATA_TOKENIZED: list[str] = tokenize(DATA)\n",
    "\n",
    "def analyze_vocab(\n",
    "\ttokenized_text: list[str],\n",
    ") -> Counter[str]:\n",
    "\n",
    "\tvocab_freq: Counter[str] = Counter(DATA_TOKENIZED)\n",
    "\tvocab_size: int = len(vocab_freq)\n",
    "\n",
    "\tprint(f\"{vocab_size = }\")\n",
    "\tprint(f\"{vocab_freq.most_common(10) = }\")\n",
    "\tprint(f\"{vocab_freq.most_common()[-10:] = }\")\n",
    "\n",
    "\t# get longest and shortest words\n",
    "\tvocab_length_sorted: list[str] = sorted(vocab_freq.keys(), key=len)\n",
    "\tprint(f\"{vocab_length_sorted[:10] = }\")\n",
    "\tprint(f\"{vocab_length_sorted[-10:] = }\")\n",
    "\n",
    "\t# plot histogram of word frequencies\n",
    "\tplt.figure(figsize=(10, 5))\n",
    "\tplt.hist(np.log10(np.array(list(vocab_freq.values()))), bins=100, log=True)\n",
    "\tplt.yscale('log')\n",
    "\tplt.xlabel(\"log(Word frequency)\")\n",
    "\tplt.ylabel(\"Number of words\")\n",
    "\tplt.show()\n",
    "\n",
    "\t# plot histogram of word lengths\n",
    "\tword_lengths = np.array([len(x) for x in vocab_freq.keys()])\n",
    "\tplt.figure(figsize=(10, 5))\n",
    "\tplt.hist(word_lengths, bins=20, log=True)\n",
    "\tplt.yscale('log')\n",
    "\tplt.xlabel(\"Word length\")\n",
    "\tplt.ylabel(\"Number of words\")\n",
    "\tplt.title(f\"Word length distribution\\nmean = {word_lengths.mean():.2f}, median = {np.median(word_lengths):.2f}\")\n",
    "\tplt.show()\n",
    "\n",
    "\treturn vocab_freq\n",
    "\n",
    "VOCAB_FREQ: Counter[str] = analyze_vocab(DATA_TOKENIZED)\n",
    "WORDS_BY_FREQ: dict[int, list[str]] = {\n",
    "\tfreq: [\n",
    "\t\tword \n",
    "\t\tfor word, word_freq in VOCAB_FREQ.items() \n",
    "\t\tif word_freq == freq\n",
    "\t]\n",
    "\tfor freq in set(VOCAB_FREQ.values())\n",
    "}\n",
    "\n",
    "for freq in range(1, 4):\n",
    "\tprint(f\"{len(WORDS_BY_FREQ[freq])} words with frequency {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# print a random sample of the once-used words\n",
    "print(np.random.choice(WORDS_BY_FREQ[1], size=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding & Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# sorted by frequency\n",
    "VOCAB_ARR: list[str] = [word for word, _ in VOCAB_FREQ.most_common()]\n",
    "VOCAB_DICT: dict[str, int] = {word: i for i, word in enumerate(VOCAB_ARR)}\n",
    "\n",
    "def encode(\n",
    "\ttext: str|list[str],\n",
    ") -> Int[np.ndarray, \"n_tokens\"]:\n",
    "\tif isinstance(text, str):\n",
    "\t\ttext = tokenize(text)\n",
    "\treturn np.array([VOCAB_DICT[word] for word in text])\n",
    "\n",
    "def decode(\n",
    "\tencoded_text: list[int],\n",
    ") -> str:\n",
    "\treturn ' '.join(VOCAB_ARR[i] for i in encoded_text)\n",
    "\n",
    "DATA_ENCODED: Int[np.ndarray, \"n_tokens\"] = encode(DATA)\n",
    "\n",
    "print(f\"{DATA_ENCODED[:10] = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train_markov_chain(\n",
    "\tvocab_size: int,\n",
    "\tdata_encoded: Int[np.ndarray, \"n_tokens\"],\n",
    ") -> Float[np.ndarray, \"d_vocab d_vocab\"]:\n",
    "\n",
    "\t# initialize matrix\n",
    "\tchain: Float[np.ndarray, \"d_vocab d_vocab\"] = np.zeros(\n",
    "\t\t(vocab_size, vocab_size),\n",
    "\t\tdtype=np.float32,\n",
    "\t)\n",
    "\n",
    "\t# count transitions\n",
    "\tfor i in range(len(data_encoded) - 1):\n",
    "\t\tchain[data_encoded[i], data_encoded[i+1]] += 1\n",
    "\n",
    "\t# normalize\n",
    "\tchain = chain / chain.sum(axis=1, keepdims=True)\n",
    "\n",
    "\treturn chain\n",
    "\n",
    "CHAIN: Float[np.ndarray, \"d_vocab d_vocab\"] = train_markov_chain(\n",
    "\tvocab_size=len(VOCAB_ARR),\n",
    "\tdata_encoded=DATA_ENCODED,\n",
    ")\n",
    "\n",
    "print(f\"{CHAIN.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def plot_chain_subset(\n",
    "\tchain: Float[np.ndarray, \"d_vocab d_vocab\"],\n",
    "\tvocab_dict: dict[str, int],\n",
    "\tvocab_subset: list[str],\n",
    "\texponentiate: int = 1,\n",
    ") -> None:\n",
    "\tfig, ax = plt.subplots(figsize=(10, 10))\n",
    "\t\n",
    "\t# get indices of words in subset\n",
    "\tvocab_subset_idx: list[int] = [vocab_dict[word] for word in vocab_subset]\n",
    "\n",
    "\t# plot subset of chain\n",
    "\tchain_subset: Float[np.ndarray, \"d_vocab_subset d_vocab_subset\"] = chain[np.ix_(vocab_subset_idx, vocab_subset_idx)]\n",
    "\n",
    "\tif exponentiate != 1:\n",
    "\t\tchain_subset = np.linalg.matrix_power(chain_subset, exponentiate)\n",
    "\n",
    "\timg = ax.matshow(chain_subset)\n",
    "\tax.set_title(\"Transition probabilities\")\n",
    "\tax.set_xlabel(\"Next word\")\n",
    "\tax.set_ylabel(\"Previous word\")\n",
    "\t# label axes with words\n",
    "\tax.set_xticks(range(len(vocab_subset)))\n",
    "\tax.set_xticklabels(vocab_subset, rotation=90)\n",
    "\tax.set_yticks(range(len(vocab_subset)))\n",
    "\tax.set_yticklabels(vocab_subset)\n",
    "\t\n",
    "\t# add colorbar\n",
    "\tfig.colorbar(img)\n",
    "\t\n",
    "\tplt.show()\n",
    "\n",
    "plot_chain_subset(\n",
    "\tchain=CHAIN,\n",
    "\tvocab_dict=VOCAB_DICT,\n",
    "\tvocab_subset=VOCAB_ARR[:50],\n",
    "\t# exponentiate=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def chain_generate(\n",
    "\tcontext: str|list[str]|Int[np.ndarray, \"n_tokens\"],\n",
    "\tchain: Float[np.ndarray, \"d_vocab d_vocab\"],\n",
    "\tmax_new_tokens: int,\n",
    "\tvocab_dict: dict[str, int],\n",
    "\treturn_encoded: bool = False,\n",
    ") -> str:\n",
    "\tif isinstance(context, str):\n",
    "\t\tcontext = tokenize(context, process=True)\n",
    "\tif isinstance(context, list):\n",
    "\t\tcontext = encode(context)\n",
    "\n",
    "\t# for easier appending\n",
    "\tcontext = context.tolist()\n",
    "\n",
    "\t# get the probabilities of the next word\n",
    "\tfor i in range(max_new_tokens):\n",
    "\t\tcontext.append(\n",
    "\t\t\tnp.random.choice(\n",
    "\t\t\t\tlen(vocab_dict),\n",
    "\t\t\t\tp=chain[context[-1]],\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t\n",
    "\tif return_encoded:\n",
    "\t\treturn context\n",
    "\telse:\n",
    "\t\treturn decode(context)\n",
    "\n",
    "chain_generate(\n",
    "\tcontext=\"of the\",\n",
    "\tchain=CHAIN,\n",
    "\tmax_new_tokens=20,\n",
    "\tvocab_dict=VOCAB_DICT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced tokenization\n",
    "\n",
    "In reality, language is really complex. We also don't want to strip things like capitalization or punctuation, and we want to be able to create compound words out of root words, suffixes, and prefixed. Although probably far from optimal, the current most popular algorithm used to come up with a token vocabulary is *byte-pair encoding.*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_most_frequent_pair(vocab):\n",
    "    return max(vocab, key=vocab.get)\n",
    "\n",
    "def byte_pair_encoding(tokens, num_merges):\n",
    "    \"\"\"\n",
    "    Implement Byte Pair Encoding.\n",
    "\n",
    "    Args:\n",
    "    tokens (list of str): The input tokens to be compressed.\n",
    "    num_merges (int): Number of merge operations to perform.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Compressed tokens and the token vocabulary mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize vocabulary with token pairs\n",
    "    token_pairs = [tokens[i:i+2] for i in range(len(tokens)-1)]\n",
    "    vocab = Counter(map(tuple, token_pairs))\n",
    "\n",
    "    # Initialize the token vocabulary\n",
    "    token_vocab = {}\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        most_frequent = get_most_frequent_pair(vocab)\n",
    "\n",
    "        # If frequency is less than 2, break the loop\n",
    "        if vocab[most_frequent] < 2:\n",
    "            break\n",
    "\n",
    "        # Create a new token\n",
    "        new_token = f'Z{(26 - i) % 26}'  # ZYX... cycle\n",
    "        token_vocab[new_token] = ''.join(most_frequent)\n",
    "\n",
    "        # Update tokens by merging the most frequent pair\n",
    "        new_tokens = []\n",
    "        skip_next = False\n",
    "        for j in range(len(tokens)):\n",
    "            if skip_next:\n",
    "                skip_next = False\n",
    "                continue\n",
    "\n",
    "            if j < len(tokens) - 1 and tokens[j] == most_frequent[0] and tokens[j + 1] == most_frequent[1]:\n",
    "                new_tokens.append(new_token)\n",
    "                skip_next = True\n",
    "            else:\n",
    "                new_tokens.append(tokens[j])\n",
    "\n",
    "        tokens = new_tokens\n",
    "\n",
    "        # Update the vocabulary\n",
    "        new_vocab = Counter()\n",
    "        for j in range(len(tokens) - 1):\n",
    "            new_vocab[tuple(tokens[j:j+2])] += 1\n",
    "        vocab = new_vocab\n",
    "\n",
    "    return tokens, token_vocab\n",
    "\n",
    "# Test the function with list of tokens\n",
    "test_tokens = list(\"ababcbabcdcd\")\n",
    "compressed_tokens, token_vocab = byte_pair_encoding(test_tokens, 10)\n",
    "\n",
    "compressed_tokens, token_vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams, Bigrams, and N-grams\n",
    "\n",
    "A fundamental problem that remains is that everything we've build so far looks only at the most recent token (whether that be a word or a character).\n",
    "\n",
    "In reality, human language does not satisfy the markov property. So, we want to be able to give the system a *sequence* of tokens.\n",
    "\n",
    "Recall our previous formalization of a markov chain as a neural network $h$:\n",
    "\n",
    "$$ h_\\theta(x_t) = M x_t $$\n",
    "$$ \\hat{\\theta} = \\argmin_\\theta \\mathbb{E}_{x_t, x_{t+1} \\in D} \\Vert h_\\theta(x_t), x_{t+1} \\Vert $$\n",
    "\n",
    "where $x_t$ is a one-hot vector encoding the current token.\n",
    "\n",
    "Recall that this means that for a vocabulary size of $d_v$, we have that $M \\in R^{d_v \\times d_v}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph LR;\n",
    "\tx0([\"x[t]\"])\n",
    "\tM[[M]]\n",
    "\tx1([\"x[t+1]\"])\n",
    "\tx0-->M;\n",
    "\tM-->x1;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tempting thing is just to include another state in the input to the matrix. This gives us a matrix $M \\in R^{d_v \\times d_v^2}$, where $d_v^2$ is the size of the vocabulary of all possible *bigrams*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph LR;\n",
    "\tx0([\"x[t-1]\"])\n",
    "\tx1([\"x[t]\"])\n",
    "\tM[[M]]\n",
    "\txn([\"x[t+1]\"])\n",
    "\tx0-->M;\n",
    "\tx1-->M;\n",
    "\tM-->xn;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can actually extend this to what is known as an $n$-gram model, where we look at the previous $n$ tokens. This gives us a matrix $M \\in R^{d_v \\times d_v^n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph LR;\n",
    "\txtn([\"x[t-n]\"])\n",
    "\txdd(((...)))\n",
    "\txt1([\"x[t-1]\"])\n",
    "\txt([\"x[t]\"])\n",
    "\txk([\"x[t+1]\"])\n",
    "\tM[[M]]\n",
    "\txtn-->M;\n",
    "\txdd-->M;\n",
    "\txt1-->M;\n",
    "\txt-->M;\n",
    "\tM-->xk;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say we want to try something conservative like a sequence length of 128, with a vocabulary size of 30 (letters, spaces, some basic punctuation). This gives us a matrix $M \\in R^{30 \\times 30^{128}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mat_elements = 30 * (30 ** 128)\n",
    "print(f\"{mat_elements = }\")\n",
    "print(f\"{mat_elements = :.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **vastly** beyond the capabilities of any computer to store -- its more than the *number of particles in the universe, **squared***\n",
    "\n",
    "\n",
    "Let's say we have a really good tokenization scheme and don't care about obscure words, and we only have $d_v = 16000$ tokens in our vocabulary. With an average word length of 8, let's say we need 16 tokens to approximate 128 characters. This gives us a matrix $M \\in R^{16000 \\times 16000^{16}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mat_elements_tokenized = 16000 * (16000 ** 16)\n",
    "print(f\"{mat_elements_tokenized = }\")\n",
    "print(f\"{mat_elements_tokenized = :.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so... a bit better, but still unimaginably huge (about 18 orders of magnitude more than the total mass of the observable universe in kg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking back to our $n$-gram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph LR;\n",
    "\txtn([\"x[t-n]\"])\n",
    "\txdd(((...)))\n",
    "\txt1([\"x[t-1]\"])\n",
    "\txt([\"x[t]\"])\n",
    "\txk([\"x[t+1]\"])\n",
    "\tM[[M]]\n",
    "\txtn-->M;\n",
    "\txdd-->M;\n",
    "\txt1-->M;\n",
    "\txt-->M;\n",
    "\tM-->xk;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what if we gave the model the *prior* that each token needs the same sort of preprocessing? We call this an *embedding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph LR;\n",
    "\txtn([\"x[t-n]\"])\n",
    "\txdd(((...)))\n",
    "\txt1([\"x[t-1]\"])\n",
    "\txt([\"x[t]\"])\n",
    "\txk([\"x[t+1]\"])\n",
    "\tE[[Embedding]]\n",
    "\tM[[M]]\n",
    "\txtn-->E;\n",
    "\txdd-->E;\n",
    "\txt1-->E;\n",
    "\txt-->E;\n",
    "\tE-->M;\n",
    "\tM-->xk;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can compute that for any reasonable embedding size, the number of parameters in our model is still going to be huge -- the problem is that we are learning a *unique* map for each position in the sequence -- just like our dense net was learning a unique map for each pixel in the image. What if we structured the network so that it could *share* parameters across positions in the sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph LR;\n",
    "\txtn([\"x[t-n]\"])\n",
    "\txdd(((...)))\n",
    "\txt1([\"x[t-1]\"])\n",
    "\txt([\"x[t]\"])\n",
    "\txk([\"x[t+1]\"])\n",
    "\tEtn[[Embedding]]\n",
    "\tEdd[[Embedding]]\n",
    "\tEt1[[Embedding]]\n",
    "\tEt[[Embedding]]\n",
    "\txtn-->Etn;\n",
    "\txdd-->Edd;\n",
    "\txt1-->Et1;\n",
    "\txt-->Et;\n",
    "\thtn[[h]];\n",
    "\thdd[[h]];\n",
    "\tht1[[h]];\n",
    "\tht[[h]];\n",
    "\tEtn-->htn;\n",
    "\thtn--\"q[t-n]\"-->hdd;\n",
    "\tEdd-->hdd;\n",
    "\thdd--\"q[...]\"-->ht1;\n",
    "\tEt1-->ht1;\n",
    "\tht1--\"q[t-1]\"-->ht;\n",
    "\tEt-->ht;\n",
    "\tht-->xk;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we term a *Recurrent Neural Network* -- it has a hidden state $q$, computed by some network $h$, which is a function both of the previous hidden state and the current input:\n",
    "\n",
    "$$\n",
    "\t\\texttt{RNN}([x_{t+1}, \\ldots, x_{t-n}]) = h(x_{t+1}, q_{t})\n",
    "\t\\qquad\n",
    "\tq_{t} = h(x_{t}, q_{t-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "# Defining the BasicRNN class\n",
    "class BasicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, layer_sizes: List[int]):\n",
    "        super(BasicRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # keep track of the hidden state\n",
    "\n",
    "        # Create the dense layers\n",
    "        # TODO\n",
    "\n",
    "        # de-embeddings\n",
    "        # TODO\n",
    "\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO\n",
    "        # randomly init hidden state\n",
    "        # pass all tokens through embedding\n",
    "        # update hidden state sequentially\n",
    "        # produce output from hidden state for all tokens\n",
    "\n",
    "    def generate(self, start_token: int, max_length: int = 100) -> List[int]:\n",
    "        # TODO\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = BasicRNN(100, 50, 100, [200, 150])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy training data\n",
    "# Using a simple sequence of numbers as dummy text\n",
    "dummy_text = list(range(20))  # Simple sequence of numbers\n",
    "input_tokens = torch.tensor(dummy_text[:-1])  # All tokens except the last\n",
    "target_tokens = torch.tensor(dummy_text[1:])  # All tokens except the first\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "output = model(input_tokens)\n",
    "loss = F.nll_loss(output, target_tokens)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare\n",
    "\n",
    "you might need to install the beautifulsoup4 package first:\n",
    "```\n",
    "pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_shakespeare_data(\n",
    "\t\turl: str = \"https://github.com/TheMITTech/shakespeare/zipball/master\",\n",
    "\t\tpattern: str = \"**/full.html\",\n",
    "        data_temp: Path|str = \"../data/shakespeare_data\",\n",
    "\t\tstrip_html: bool = True,\n",
    "\t\tchars_threshold: int|None = None,\n",
    "\t) -> dict[str, str]:\n",
    "    \n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_cache: Path = data_temp / \"processed.json\"\n",
    "\n",
    "\t# read from cache if it exists\n",
    "\tif data_cache.exists():\n",
    "\t\tprint(f\"Reading from cache...\")\n",
    "\t\twith open(data_cache, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata: dict[str,str] = json.load(file)\n",
    "\t\tprint(f\"Read {len(data)} files from cache with {sum(len(v) for v in data.values())} characters\")\n",
    "\t\treturn data\n",
    "\n",
    "\t# Download the zip file\n",
    "\tprint(f\"Downloading repo...\")\n",
    "\tresponse = requests.get(url)\n",
    "\tresponse.raise_for_status()  # Ensure that the download was successful\t\n",
    "\n",
    "\t# Extract the zip file into a temporary directory\n",
    "\tprint(f\"Extracting zip file...\")\n",
    "\twith zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "\t\tzip_ref.extractall(data_temp)\n",
    "\n",
    "\t# Find all 'full.html' files\n",
    "\tfull_html_files = list(data_temp.glob(pattern))\n",
    "\n",
    "\t# Read contents of each 'full.html' file\n",
    "\tdata: dict[str, str] = dict()\n",
    "\t# open each file\n",
    "\tprint(f\"Reading files...\\n\")\n",
    "\tchars_count: int = 0\n",
    "\tfor file_path in full_html_files:\n",
    "\t\tprint(f\"\\t{file_path.as_posix()}\")\n",
    "\t\twith open(file_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\t# read the raw html\n",
    "\t\t\tcontent: str = file.read()\n",
    "\n",
    "\t\t\t# turn it into plain text if requested\n",
    "\t\t\tif strip_html:\n",
    "\t\t\t\tsoup: BeautifulSoup = BeautifulSoup(content, 'html.parser')\n",
    "\t\t\t\tcontent = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "\t\t\t# store it in the dictionary\n",
    "\t\t\tdata[file_path.as_posix()] = content\n",
    "\n",
    "\t\t\tchars_count += len(content)\n",
    "\t\t\tprint(f\"\\t\\t{chars_count} characters read\")\n",
    "\t\t\tif chars_threshold is not None and chars_count > chars_threshold:\n",
    "\t\t\t\tbreak\n",
    "\t\n",
    "\t# save to cache\n",
    "\tprint(f\"Saving to cache...\")\n",
    "\twith open(data_cache, 'w', encoding='utf-8') as file:\n",
    "\t\tjson.dump(data, file, indent=4)\n",
    "\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "DATA: dict[str, str] = get_shakespeare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'collections' has no attribute 'MutableMapping'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(DATA[list(DATA.keys())[0]][:1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
