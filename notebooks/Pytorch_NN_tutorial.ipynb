{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6mkxw3UTkmDa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Data Loader"
      ],
      "metadata": {
        "id": "3Z0Qn6BjlciJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 100\n",
        "input_dim = 3\n",
        "output_dim = 3\n",
        "batch_size = 10\n",
        "\n",
        "x = torch.randn(n_samples, input_dim)\n",
        "y = torch.randn(n_samples, output_dim)\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset = TensorDataset(x, y)\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "U5Dx4_a2mfhR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vm7QQ1vkmDc"
      },
      "source": [
        "### 2. Define a Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wS6jnuKYkmDc"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 20)\n",
        "        self.fc2 = nn.Linear(20, 20)\n",
        "        self.fc3 = nn.Linear(20, output_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# initialize the network\n",
        "net = Net(input_dim, output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA8LfobBkmDc"
      },
      "source": [
        "### 3. Define a Loss function and optimizer\n",
        "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n2HOPsMLkmDc"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8CktlakmDc"
      },
      "source": [
        "### 4. Train the network\n",
        "\n",
        "This is when things start to get interesting.\n",
        "We simply have to loop over our data iterator, and feed the inputs to the\n",
        "network and optimize.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bfX1_tVekmDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21da0fc2-bcef-4722-d969-f09fcd14a345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0,  loss: 1.076159\n",
            "iter: 1,  loss: 1.062353\n",
            "iter: 2,  loss: 1.052567\n",
            "iter: 3,  loss: 1.047058\n",
            "iter: 4,  loss: 1.039210\n",
            "iter: 5,  loss: 1.034401\n",
            "iter: 6,  loss: 1.029787\n",
            "iter: 7,  loss: 1.026330\n",
            "iter: 8,  loss: 1.022052\n",
            "iter: 9,  loss: 1.019423\n",
            "iter: 10,  loss: 1.016291\n",
            "iter: 11,  loss: 1.013432\n",
            "iter: 12,  loss: 1.010991\n",
            "iter: 13,  loss: 1.008025\n",
            "iter: 14,  loss: 1.005992\n",
            "iter: 15,  loss: 1.003428\n",
            "iter: 16,  loss: 1.001189\n",
            "iter: 17,  loss: 0.998564\n",
            "iter: 18,  loss: 0.996285\n",
            "iter: 19,  loss: 0.994500\n",
            "iter: 20,  loss: 0.992896\n",
            "iter: 21,  loss: 0.990385\n",
            "iter: 22,  loss: 0.989453\n",
            "iter: 23,  loss: 0.986186\n",
            "iter: 24,  loss: 0.984480\n",
            "iter: 25,  loss: 0.982497\n",
            "iter: 26,  loss: 0.980455\n",
            "iter: 27,  loss: 0.977989\n",
            "iter: 28,  loss: 0.975705\n",
            "iter: 29,  loss: 0.973445\n",
            "iter: 30,  loss: 0.971399\n",
            "iter: 31,  loss: 0.969911\n",
            "iter: 32,  loss: 0.966612\n",
            "iter: 33,  loss: 0.965049\n",
            "iter: 34,  loss: 0.963404\n",
            "iter: 35,  loss: 0.960482\n",
            "iter: 36,  loss: 0.958250\n",
            "iter: 37,  loss: 0.956955\n",
            "iter: 38,  loss: 0.954187\n",
            "iter: 39,  loss: 0.953281\n",
            "iter: 40,  loss: 0.949416\n",
            "iter: 41,  loss: 0.948859\n",
            "iter: 42,  loss: 0.946233\n",
            "iter: 43,  loss: 0.943575\n",
            "iter: 44,  loss: 0.940830\n",
            "iter: 45,  loss: 0.941306\n",
            "iter: 46,  loss: 0.936111\n",
            "iter: 47,  loss: 0.935773\n",
            "iter: 48,  loss: 0.932900\n",
            "iter: 49,  loss: 0.929416\n",
            "iter: 50,  loss: 0.928171\n",
            "iter: 51,  loss: 0.924727\n",
            "iter: 52,  loss: 0.924039\n",
            "iter: 53,  loss: 0.920374\n",
            "iter: 54,  loss: 0.919030\n",
            "iter: 55,  loss: 0.916950\n",
            "iter: 56,  loss: 0.915169\n",
            "iter: 57,  loss: 0.912304\n",
            "iter: 58,  loss: 0.909842\n",
            "iter: 59,  loss: 0.908317\n",
            "iter: 60,  loss: 0.905081\n",
            "iter: 61,  loss: 0.903993\n",
            "iter: 62,  loss: 0.901572\n",
            "iter: 63,  loss: 0.899619\n",
            "iter: 64,  loss: 0.897873\n",
            "iter: 65,  loss: 0.894995\n",
            "iter: 66,  loss: 0.894180\n",
            "iter: 67,  loss: 0.890498\n",
            "iter: 68,  loss: 0.889916\n",
            "iter: 69,  loss: 0.887404\n",
            "iter: 70,  loss: 0.885312\n",
            "iter: 71,  loss: 0.883530\n",
            "iter: 72,  loss: 0.881773\n",
            "iter: 73,  loss: 0.878985\n",
            "iter: 74,  loss: 0.876265\n",
            "iter: 75,  loss: 0.875438\n",
            "iter: 76,  loss: 0.872881\n",
            "iter: 77,  loss: 0.873437\n",
            "iter: 78,  loss: 0.869053\n",
            "iter: 79,  loss: 0.867819\n",
            "iter: 80,  loss: 0.865838\n",
            "iter: 81,  loss: 0.864245\n",
            "iter: 82,  loss: 0.861927\n",
            "iter: 83,  loss: 0.860047\n",
            "iter: 84,  loss: 0.858558\n",
            "iter: 85,  loss: 0.857532\n",
            "iter: 86,  loss: 0.855448\n",
            "iter: 87,  loss: 0.853205\n",
            "iter: 88,  loss: 0.852267\n",
            "iter: 89,  loss: 0.848420\n",
            "iter: 90,  loss: 0.848005\n",
            "iter: 91,  loss: 0.846317\n",
            "iter: 92,  loss: 0.843784\n",
            "iter: 93,  loss: 0.842672\n",
            "iter: 94,  loss: 0.841807\n",
            "iter: 95,  loss: 0.840055\n",
            "iter: 96,  loss: 0.837331\n",
            "iter: 97,  loss: 0.837594\n",
            "iter: 98,  loss: 0.836955\n",
            "iter: 99,  loss: 0.832840\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "max_epochs = 100\n",
        "\n",
        "for epoch in range(max_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      # get the inputs; data is a list of [inputs, labels]\n",
        "      x_batch, y_true_batch = data\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      y_pred_batch = net(x_batch)\n",
        "      loss = criterion(y_pred_batch, y_true_batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()*batch_size\n",
        "\n",
        "\n",
        "    ave_loss = running_loss/n_samples\n",
        "    # print statistics\n",
        "    print('iter: %d,' %epoch, ' loss: %f' % ave_loss)\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TVK4uUlCmz-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}